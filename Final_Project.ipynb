{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Review Analysis\n",
    "## By. Andrew Simmons & Jingnan Jin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Reference\n",
    "0. marketplace\n",
    "1. customer_id\n",
    "2. review_id\n",
    "3. product_id\n",
    "4. product_parent\n",
    "5. product_title\n",
    "6. product_category\n",
    "7. star_rating - [1-5]\n",
    "8. helpful_votes\n",
    "9. total_votes\n",
    "10. vine\n",
    "11. verified_purchase\n",
    "12. review_headline\n",
    "13. review_body\n",
    "14. review_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2/3 Support\n",
    "from __future__ import nested_scopes, generators, division, absolute_import, with_statement, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid Modes: TEST or PROD\n",
    "MODE = \"TEST\"\n",
    "\n",
    "# Valid Modes: LOCAL or S3\n",
    "DATA_MODE = \"LOCAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#import html\n",
    "import math\n",
    "from operator import itemgetter\n",
    "#from pathlib import Path\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if MODE == \"TEST\":\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "\n",
    "    # create entry points to spark\n",
    "    try:\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "    \n",
    "if MODE == \"TEST\":\n",
    "    conf = SparkConf().setAppName(\"AmazonCustomerReviewAnalysis\").setMaster(\"local[*]\")\n",
    "elif MODE == \"PROD\":\n",
    "    conf = SparkConf().setAppName(\"AmazonCustomerReviewAnalysis\")\n",
    "    \n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "if MODE == \"TEST\" and DATA_MODE == \"S3\":\n",
    "    AWS_ACCESS_KEY_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "    AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3.awsAccessKeyId\", AWS_ACCESS_KEY_ID)\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3.awsSecretAccessKey\", AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PLOT_DIMENSIONS = (20, 10)\n",
    "USELESS_WORDS = set([\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"PROD\":\n",
    "    if DATA_MODE == \"LOCAL\":\n",
    "        dataset = spark.read.load(\"~/parquet/\", 200)\n",
    "    elif DATA_MODE == \"S3\":\n",
    "        dataset = spark.read.load(\"s3a://amazon-reviews-pds/parquet/\", 200)\n",
    "        \n",
    "    dataset = dataset.filter(dataset[\"marketplace\"] == \"US\")\n",
    "\n",
    "    categories = [row['product_category'] for row in dataset.select('product_category').distinct().collect()]\n",
    "\n",
    "    data_categories = {}\n",
    "    for category in categories:\n",
    "        data_categories[category] = dataset.filter(dataset['product_category'] == category).cache()\n",
    "elif MODE == \"TEST\":\n",
    "    # Process list of files to separate by category\n",
    "    data_files = []\n",
    "    for file in glob(\"sample_data/*.tsv.gz\"):\n",
    "        base = os.path.basename(file)\n",
    "        if base.startswith(\"amazon_reviews_us_\"):\n",
    "            data_files.append(file)\n",
    "\n",
    "\n",
    "    data_categories = defaultdict(list)\n",
    "    for file in data_files:\n",
    "        base = os.path.basename(file)\n",
    "        category_name = base[18:-13].replace(\"_\", \" \")\n",
    "        data_categories[category_name].append(file)\n",
    "    \n",
    "    # Create mapping of category names to unioned RDD\n",
    "    for key, value in data_categories.items():\n",
    "        data_categories[key] = sc.union([sc.textFile(str(file)) for file in value])\n",
    "    \n",
    "    # Remove headers from data\n",
    "    for key, value in data_categories.items():\n",
    "        data_categories[key] = value.filter(lambda x: not x.startswith(\"marketplace\"))\n",
    "    \n",
    "    # Split TSV\n",
    "    for key, value in data_categories.items():\n",
    "        data_categories[key] = value.map(lambda x: x.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All categories unioned together\n",
    "def get_unioned_data(category_data):\n",
    "    rdds = []\n",
    "    for value in data_categories.values():\n",
    "        rdds.append(value)\n",
    "\n",
    "    return sc.union(rdds)\n",
    "\n",
    "if DATA_MODE == 'LOCAL':\n",
    "    unioned_data = get_unioned_data(data_categories)\n",
    "elif DATA_MODE == 'S3':\n",
    "    unioned_data = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many records exist in each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_counts = []\n",
    "for key, value in data_categories.items():\n",
    "    record_counts.append((key, value.count()))\n",
    "\n",
    "categories, counts = zip(*sorted(record_counts, key=itemgetter(1)))\n",
    "print('How many records exist in each category?')\n",
    "print(categories)\n",
    "print(counts)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(PLOT_DIMENSIONS)\n",
    "\n",
    "# rects = ax.bar(categories, counts)\n",
    "\n",
    "# ax.set_title(\"Amazon Review Counts by Category\")\n",
    "# ax.set_xlabel(\"Product Category\")\n",
    "# ax.set_ylabel(\"Review Counts\")\n",
    "\n",
    "# for tick in ax.get_xticklabels():\n",
    "#     tick.set_rotation(90)\n",
    "    \n",
    "# filename = 'records_in_each_category'\n",
    "# fig.savefig(filename + '.pdf')\n",
    "# fig.savefig(filename + '.png')\n",
    "\n",
    "# if MODE == \"PROD\":\n",
    "#     filename = 'records_in_each_category'\n",
    "#     fig.savefig(filename + '.pdf')\n",
    "#     fig.savefig(filename + '.png')\n",
    "\n",
    "print(\"Total number of reviews in dataset: {}\".format(sum(counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall, how satisfied are customers of each product category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_review_by_category = []\n",
    "for key, value in data_categories.items():\n",
    "    average_review_by_category.append((key, value.select(mean(data_categories[key]['star_rating'])).collect()[0][0]))\n",
    "\n",
    "categories, ratings = zip(*sorted(average_review_by_category, key=itemgetter(1)))\n",
    "print('Overall, how satisfied are customers of each product category?')\n",
    "print(categories)\n",
    "print(ratings)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(PLOT_DIMENSIONS)\n",
    "\n",
    "# rects = ax.bar(categories, ratings)\n",
    "\n",
    "# ax.set_title(\"Average Product Category Rating\")\n",
    "# ax.set_xlabel(\"Product Category\")\n",
    "# ax.set_ylabel(\"Average Rating\")\n",
    "\n",
    "# for tick in ax.get_xticklabels():\n",
    "#     tick.set_rotation(90)\n",
    "    \n",
    "# if MODE == \"PROD\":\n",
    "#     filename = 'satisfaction_in_each_category'\n",
    "#     fig.savefig(filename + '.pdf')\n",
    "#     fig.savefig(filename + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the distribution of review scores change between product category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_distributions = {}\n",
    "for key, value in data_categories.items():\n",
    "    one_star = value.filter('star_rating == 1').count()\n",
    "    two_star = value.filter('star_rating == 2').count()\n",
    "    three_star = value.filter('star_rating == 3').count()\n",
    "    four_star = value.filter('star_rating == 4').count()\n",
    "    five_star = value.filter('star_rating == 5').count()\n",
    "    review_distributions[key] = [(1, one_star),\n",
    "                                 (2, two_star),\n",
    "                                 (3, three_star),\n",
    "                                 (4, four_star),\n",
    "                                 (5, five_star),\n",
    "                                ]\n",
    "\n",
    "num_columns = 3\n",
    "\n",
    "print('How does the distribution of review scores change between product category?')\n",
    "print(review_distributions)\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=math.ceil(len(review_distributions.keys()) / num_columns),\n",
    "#                        ncols=num_columns)\n",
    "# fig.set_size_inches((20, 100))\n",
    "# ax = ax.flatten()\n",
    "\n",
    "# for i, (key, value) in enumerate(review_distributions.items()):\n",
    "#     stars, counts = zip(*value)\n",
    "    \n",
    "#     rects = ax[i].bar(stars, counts)\n",
    "    \n",
    "#     # TODO: Calculate mean more efficiantly\n",
    "#     total = 0\n",
    "#     for j in range(len(stars)):\n",
    "#         total += stars[j] * counts[j]\n",
    "#     mean = total / sum(counts)\n",
    "    \n",
    "#     # TODO: Include a median line\n",
    "#     mean_line = ax[i].axvline(mean, color='red', linestyle='--')\n",
    "#     ax[i].legend([mean_line], ['Mean Category Rating'])\n",
    "    \n",
    "#     ax[i].set_title('Ratings Frequency Distribution for {} category'.format(key))\n",
    "#     ax[i].set_xlabel('Star Rating')\n",
    "#     ax[i].set_ylabel('Rating Count')\n",
    "\n",
    "# if MODE == \"PROD\":\n",
    "#     filename = 'review_scores_distribution_by_category'\n",
    "#     fig.savefig(filename + '.pdf')\n",
    "#     fig.savefig(filename + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What words are most used in each category at each rating for review headlines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category: {\n",
    "#     1: [n most common words],\n",
    "#     2: ...\n",
    "# },\n",
    "\n",
    "# DEFAULT_REVIEWS = {\"One Star\", \"Two Stars\", \"Three Stars\", \"Four Stars\", \"Five Stars\"}\n",
    "\n",
    "# resulting_word_frequencies = defaultdict(dict)\n",
    "\n",
    "# for key, value in data_categories.items():\n",
    "\n",
    "#     rating_words = {}\n",
    "\n",
    "#     for i in POSSIBLE_RATINGS:\n",
    "\n",
    "#         reviews = value.filter(lambda x: x[7] == i)\n",
    "\n",
    "#         headlines = (\n",
    "#             reviews.map(lambda x: x[12])\n",
    "#             .filter(remove_default_headlines)\n",
    "#             .map(preprocess)\n",
    "#         )\n",
    "\n",
    "#         headline_words = (\n",
    "#             headlines.flatMap(lambda x: x.split())\n",
    "#             .filter(remove_useless_words)\n",
    "#             .filter(remove_empty_words)\n",
    "#             .filter(remove_censored_swear_words)\n",
    "#             .filter(remove_words_without_alphanumeric)\n",
    "#             .filter(remove_single_letter_words)\n",
    "#         )\n",
    "\n",
    "#         word_frequencies = (\n",
    "#             headline_words.map(lambda x: (x, 1))\n",
    "#             .reduceByKey(lambda x, y: x + y)\n",
    "#             .sortBy(lambda x: x[1], ascending=False)\n",
    "#             .map(lambda x: x[0])\n",
    "#         )\n",
    "\n",
    "#         resulting_word_frequencies[key][i] = word_frequencies.take(3)\n",
    "\n",
    "# # Output results\n",
    "# for category in resulting_word_frequencies.keys():\n",
    "\n",
    "#     print(\"{}:\".format(category))\n",
    "\n",
    "#     for rating in POSSIBLE_RATINGS:\n",
    "\n",
    "#         words = resulting_word_frequencies[category][rating]\n",
    "\n",
    "#         print(\"  {}: {}\".format(rating, list(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What words are most used in each category at each rating for review bodies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a9bc4004e385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         )\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mresulting_word_frequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_frequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Output results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BigDataEnvironment/venv/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BigDataEnvironment/venv/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BigDataEnvironment/venv/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/Documents/BigDataEnvironment/venv/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/BigDataEnvironment/venv/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "POSSIBLE_RATINGS = [1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "def remove_default_headlines(headline):\n",
    "    \"\"\"Remove default headlines\"\"\"\n",
    "    if headline in DEFAULT_REVIEWS:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocess(headline):\n",
    "    # Remove HTML breaks\n",
    "    headline = re.sub(u\"<br />\", \" \", headline)\n",
    "    # Convert multiple spaces to a single space\n",
    "    headline = re.sub(u\"\\\\s+\", \" \", headline, flags=re.I)\n",
    "    # Remove punctuation that is found at the end of words\n",
    "    headline = re.sub(u\"[,.!?]\", \"\", headline)\n",
    "    # Remove apostrophies as to group together both spellings of a word\n",
    "    headline = re.sub(u\"'\", \"\", headline)\n",
    "    return headline.lower()\n",
    "\n",
    "\n",
    "def remove_useless_words(headline_word):\n",
    "    \"\"\"Remove stop words and elipses\"\"\"\n",
    "    if headline_word in USELESS_WORDS:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_empty_words(headline_word):\n",
    "    if len(headline_word) == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_censored_swear_words(headline_word):\n",
    "    \"\"\"Amazon appears to have censored swear words by keeping the first character\n",
    "    and replacing the other characters with astrisks. These are not useful to us.\n",
    "    \"\"\"\n",
    "    if re.match(u\"^[a-z]\\\\*+$\", headline_word) is not None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_words_without_alphanumeric(headline_word):\n",
    "    if re.match(u\"[a-z0-9]\", headline_word) is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# TODO: This doesn't seem to work 100% of the time\n",
    "def remove_numbers(headline_word):\n",
    "    \"\"\"Remove words that consist of nothing but numbers\"\"\"\n",
    "    if re.match(u\"^[0-9]+$\", headline_word) is not None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_single_letter_words(headline_word):\n",
    "    if len(headline_word) == 1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "resulting_word_frequencies = defaultdict(dict)\n",
    "for key, value in data_categories.items():\n",
    "#     value = value[['star_rating', 'review_body']].rdd.map(tuple)\n",
    "    rating_words = {}\n",
    "    for i in POSSIBLE_RATINGS:\n",
    "        reviews = value.filter(lambda x: x[0] == i)\n",
    "        bodies = reviews.map(lambda x: x[1]).map(preprocess)\n",
    "        \n",
    "        print(reviews.take(3))\n",
    "        \n",
    "        body_words = (\n",
    "            bodies.flatMap(lambda x: x.split())\n",
    "            .filter(remove_useless_words)\n",
    "            .filter(remove_empty_words)\n",
    "            .filter(remove_censored_swear_words)\n",
    "            .filter(remove_words_without_alphanumeric)\n",
    "            .filter(remove_single_letter_words)\n",
    "        )\n",
    "        \n",
    "        word_frequencies = (\n",
    "            body_words.map(lambda x: (x, 1))\n",
    "            .reduceByKey(lambda x, y: x + y)\n",
    "            .sortBy(lambda x: x[1], ascending=False)\n",
    "            .map(lambda x: x[0])\n",
    "        )\n",
    "        resulting_word_frequencies[key][i] = word_frequencies.take(10)\n",
    "\n",
    "# Output results\n",
    "for category in resulting_word_frequencies.keys():\n",
    "    print(\"{}:\".format(category))\n",
    "    for rating in POSSIBLE_RATINGS:\n",
    "        words = resulting_word_frequencies[category][rating]\n",
    "        print(\"  {}: {}\".format(rating, list(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the average rating in a category change between verified and unverified purchasers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratings = []\n",
    "for key, value in data_categories.items():\n",
    "    #value.select(mean(data_categories[key]['star_rating'])).collect()[0][0]\n",
    "    all_ratings_mean = value.select(mean(value['star_rating'])).collect()[0][0]\n",
    "    verified_ratings_mean = value.filter('verified_purchase == \"Y\"').select(mean(value['star_rating'])).collect()[0][0]\n",
    "    unverified_ratings_mean = value.filter('verified_purchase == \"N\"').select(mean(value['star_rating'])).collect()[0][0]\n",
    "    mean_ratings.append((key,\n",
    "                         all_ratings_mean,\n",
    "                         verified_ratings_mean,\n",
    "                         unverified_ratings_mean,\n",
    "                        ))\n",
    "#     all_ratings_mean = value.map(lambda x: x[7]).mean()\n",
    "#     verified_ratings_mean = (\n",
    "#         value.filter(lambda x: x[11] == \"Y\").map(lambda x: x[7]).mean()\n",
    "#     )\n",
    "#     unverified_ratings_mean = (\n",
    "#         value.filter(lambda x: x[11] == \"N\").map(lambda x: x[7]).mean()\n",
    "#     )\n",
    "\n",
    "#     mean_ratings.append(\n",
    "#         (key, all_ratings_mean, verified_ratings_mean, unverified_ratings_mean)\n",
    "#     )\n",
    "\n",
    "\n",
    "num_columns = 3\n",
    "\n",
    "print('How does the average rating in a category change between verified and unverified purchasers?')\n",
    "print(mean_ratings)\n",
    "\n",
    "# fig, ax = plt.subplots(\n",
    "#     nrows=math.ceil(len(mean_ratings) / num_columns), ncols=num_columns\n",
    "# )\n",
    "\n",
    "# fig.set_size_inches((20, 100))\n",
    "# ax = ax.flatten()\n",
    "\n",
    "# labels = [\"All\", \"Verified\", \"Unverified\"]\n",
    "\n",
    "# for i, category in enumerate(mean_ratings):\n",
    "#     rects = ax[i].bar(labels, category[1:], color=[\"orange\", \"blue\", \"green\"])\n",
    "\n",
    "#     ax[i].set_title('Average Rating in \"{}\" Category'.format(category[0]))\n",
    "#     ax[i].set_xlabel(\"Review Type\")\n",
    "#     ax[i].set_ylabel(\"Average Rating\")\n",
    "\n",
    "# if MODE == \"PROD\":\n",
    "#     filename = 'verified_vs_unverified_ratings'\n",
    "#     fig.savefig(filename + '.pdf')\n",
    "#     fig.savefig(filename + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"=== Differences between verified review average and unverified review average by stars ===\\n\"\n",
    ")\n",
    "\n",
    "# (category_name, average_difference)\n",
    "average_difference = []\n",
    "for category in mean_ratings:\n",
    "    category_name = category[0]\n",
    "    verified_review_average = category[2]\n",
    "    unverified_review_average = category[3]\n",
    "\n",
    "    average_difference.append(\n",
    "        (\n",
    "            category_name,\n",
    "            abs(abs(verified_review_average) - abs(unverified_ratings_mean)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Print differences by category\n",
    "for category in average_difference:\n",
    "    print(\"{}: {}\".format(category[0], category[1]))\n",
    "\n",
    "\n",
    "# Print total average differences\n",
    "_, difference = zip(*average_difference)\n",
    "print(\"Overall: {}\".format(sum(difference) / len(difference)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What review ratings do customers find the most helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star_helpful_votes = unioned_data.filter('star_rating == 1').groupBy().sum('helpful_votes').collect()[0][0]\n",
    "one_star_total_votes = unioned_data.filter('star_rating == 1').groupBy().sum('total_votes').collect()[0][0]\n",
    "two_star_helpful_votes = unioned_data.filter('star_rating == 2').groupBy().sum('helpful_votes').collect()[0][0]\n",
    "two_star_total_votes = unioned_data.filter('star_rating == 2').groupBy().sum('total_votes').collect()[0][0]\n",
    "three_star_helpful_votes = unioned_data.filter('star_rating == 3').groupBy().sum('helpful_votes').collect()[0][0]\n",
    "three_star_total_votes = unioned_data.filter('star_rating == 3').groupBy().sum('total_votes').collect()[0][0]\n",
    "four_star_helpful_votes = unioned_data.filter('star_rating == 4').groupBy().sum('helpful_votes').collect()[0][0]\n",
    "four_star_total_votes = unioned_data.filter('star_rating == 4').groupBy().sum('total_votes').collect()[0][0]\n",
    "five_star_helpful_votes = unioned_data.filter('star_rating == 5').groupBy().sum('helpful_votes').collect()[0][0]\n",
    "five_star_total_votes = unioned_data.filter('star_rating == 5').groupBy().sum('total_votes').collect()[0][0]\n",
    "\n",
    "# # (rating, (helpful_vote_percentage, 1))\n",
    "# helpful_vote_percentage = unioned_data.filter(lambda x: x[9] != 0).map(\n",
    "#     lambda x: (x[7], (x[8] / x[9], 1))\n",
    "# )\n",
    "\n",
    "# # (rating, average_helpful_vote_percentage)\n",
    "# average_helpful_vote_percentage = helpful_vote_percentage.reduceByKey(\n",
    "#     lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "# ).map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "#helpful_averages = sorted(average_helpful_vote_percentage.collect())\n",
    "\n",
    "helpful_average = [\n",
    "    (1, one_star_helpful_votes / one_star_total_votes),\n",
    "    (2, two_star_helpful_votes / two_star_total_votes),\n",
    "    (3, three_star_helpful_votes / three_star_total_votes),\n",
    "    (4, four_star_helpful_votes / four_star_total_votes),\n",
    "    (5, five_star_helpful_votes / five_star_total_votes),\n",
    "]\n",
    "\n",
    "rating, helpful_average = zip(*helpful_averages)\n",
    "\n",
    "print('What review ratings do customers find the most helpful?')\n",
    "print(rating)\n",
    "print(helpful_average)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(PLOT_DIMENSIONS)\n",
    "\n",
    "# rects = ax.bar(rating, helpful_average)\n",
    "\n",
    "# ax.set_title(\"Average Helpfulness by Review Rating\")\n",
    "# ax.set_xlabel(\"Rating\")\n",
    "# ax.set_ylabel(\"Average Helpfulness\")\n",
    "\n",
    "# if MODE == \"PROD\":\n",
    "#     filename = 'rating_helpfulness'\n",
    "#     fig.savefig(filename + '.pdf')\n",
    "#     fig.savefig(filename + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the Amazon Vine program influence customer reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unioned_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87d6f0fe18f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maverage_vine_rating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munioned_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vine == \"Y\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# average_vine_rating = (\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     unioned_data.filter(lambda x: x[10] == \"Y\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     .map(lambda x: (x[7], 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unioned_data' is not defined"
     ]
    }
   ],
   "source": [
    "vine_data = unioned_data.filter('vine == \"Y\"')\n",
    "nonvine_data = unioned_data.filter('vine == \"N\"')\n",
    "average_vine_rating = vine_data.select(mean(vine_data['star_rating'])).collect()[0][0]\n",
    "average_nonvine_rating = nonvine_data.select(mean(vine_data['star_rating'])).collect()[0][0]\n",
    "\n",
    "# average_vine_rating = (\n",
    "#     unioned_data.filter(lambda x: x[10] == \"Y\")\n",
    "#     .map(lambda x: (x[7], 1))\n",
    "#     .reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "# )\n",
    "# average_vine_rating = average_vine_rating[0] / average_vine_rating[1]\n",
    "\n",
    "# average_nonvine_rating = (\n",
    "#     unioned_data.filter(lambda x: x[10] == \"N\")\n",
    "#     .map(lambda x: (x[7], 1))\n",
    "#     .reduce(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "# )\n",
    "# average_nonvine_rating = average_nonvine_rating[0] / average_nonvine_rating[1]\n",
    "\n",
    "difference = abs(abs(average_vine_rating) - abs(average_nonvine_rating))\n",
    "\n",
    "print(\"Difference between vine and non-vine average: {}\".format(difference))\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(PLOT_DIMENSIONS)\n",
    "\n",
    "# rects = ax.bar([\"Vine\", \"Non-Vine\"], [average_vine_rating, average_nonvine_rating])\n",
    "\n",
    "# ax.set_title(\"Vine vs Non-Vine Customer Review Average\")\n",
    "# ax.set_xlabel(\"Review Type\")\n",
    "# ax.set_ylabel(\"Average Rating\")\n",
    "\n",
    "# for tick in ax.get_xticklabels():\n",
    "#     tick.set_rotation(90)\n",
    "    \n",
    "# if MODE == \"PROD\":\n",
    "#     filename = 'vine_rating_influence'\n",
    "#     fig.savefig(filename + '.pdf')\n",
    "#     fig.savefig(filename + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is it possible to predict the rating associated with a review by the sentiment of it’s words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict that the sentiment of a word can be derived by an average of the ratings of the reviews that it occured in.\n",
    "# this will act as a weight which can be used to preduct the rating of a given review\n",
    "\n",
    "def word_to_rating(x):\n",
    "    return_list = []\n",
    "\n",
    "    rating = x[0]\n",
    "    words = x[1]\n",
    "\n",
    "    for word in words:\n",
    "        return_list.append((word, rating))\n",
    "\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def clean_words(pair_rdd):\n",
    "    words = pair_rdd[1]\n",
    "\n",
    "    words = filter(remove_useless_words, words)\n",
    "    words = filter(remove_empty_words, words)\n",
    "    words = filter(remove_censored_swear_words, words)\n",
    "    words = filter(remove_words_without_alphanumeric, words)\n",
    "    words = filter(remove_single_letter_words, words)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "# (rating, [word1, word2, ...])\n",
    "rating_to_review_words = (\n",
    "    unioned_data.map(lambda x: (x[7], x[13]))\n",
    "    .mapValues(preprocess)\n",
    "    .map(lambda x: (x[0], x[1].split()))\n",
    "    .filter(clean_words)\n",
    ")\n",
    "\n",
    "# [(word1, rating), (word2, rating), ...]\n",
    "review_words_to_ratings = rating_to_review_words.flatMap(word_to_rating)\n",
    "\n",
    "# [(word1, average_rating), (word2, average_rating), ...]\n",
    "average_word_rating = (\n",
    "    review_words_to_ratings.map(lambda x: (x[0], (x[1], 1)))\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "    .map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    ")\n",
    "\n",
    "\n",
    "word_frequency = (\n",
    "    review_words_to_ratings.map(lambda x: (x[0], 1))\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "    .collectAsMap()\n",
    ")\n",
    "total_num_words = review_words_to_ratings.map(lambda x: x[0]).count()\n",
    "\n",
    "\n",
    "word_weights = average_word_rating.collectAsMap()\n",
    "\n",
    "\n",
    "def predict_rating_from_review(review):\n",
    "    words = review.split()\n",
    "\n",
    "    weights = []\n",
    "    for word in words:\n",
    "        if word in word_weights:\n",
    "            weights.append(word_weights[word])\n",
    "\n",
    "    # If none of the words in the input are in the weight set, return None instead of a prediction\n",
    "    if len(weights) == 0:\n",
    "        return None\n",
    "\n",
    "    return sum(weights) / len(weights)\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"really amazing product i love it so much absolutely incredible\",\n",
    "    \"it was alright not the best but not the worst\",\n",
    "    \"terrible i feel scammed i will return this this should be illegal\",\n",
    "    \"worst terrible refund\",\n",
    "    \"terrible\",\n",
    "    \"bad\",\n",
    "    \"good\",\n",
    "    \"perfect\",\n",
    "    \"thiswordisnotinthedataset\",\n",
    "]\n",
    "\n",
    "print(\"=== Rating predictions ===\")\n",
    "for example in examples:\n",
    "    print('Review text: \"{}\"'.format(example))\n",
    "    print(\"Predicted Rating: {}\".format(predict_rating_from_review(example)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
